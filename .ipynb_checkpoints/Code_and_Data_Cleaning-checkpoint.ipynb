{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The US Department of State produces the *Foreign Relations of the United States (FRUS)* in both print and online forms. Their online volumes are housed in a Github for public use in TEI XML <a href=\"https://github.com/HistoryAtState/frus\">here</a>. While the markup in the files depict useful identifiers such as the officials involved, the type of document, et cetera, this project requires plain text format rather than encoded documents. Using an XQuery in Oxygen XML Editor, I removed the markup and introductory information, leaving only the contents within the <text> tag. This creates a unique version of the files particular to my project. The XQuery removes the <teiHeader> which contains metadata, the front material which includes the introductory publication statements, and table of contents, and the back information such as the index. The query, which can be found <a href=\"https://drive.google.com/file/d/1MkVhOUxD4IJbdr4Hg0z7JN83tt5Sc9Se/view?usp=sharing\">here</a>, made each file into a plain text document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vector Model Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, make sure that the you are in the correct location. Run the next chunk of code to ensure that the file path ends with \"CloutierDHProject/Code_and_Data_Cleaning\".\n",
    "To \"run\" chunks of code, either press \"command-control\"/\"command-return\" or press \"Run\" above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"tidyverse\")\n",
    "install.packages(\"tidytext\")\n",
    "install.packages(\"magrittr\")\n",
    "install.packages(\"devtools\")\n",
    "install.packages(\"tsne\")\n",
    "install.packages(\"usethis\")\n",
    "install.packages(\"SnowballC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidytext)\n",
    "library(magrittr)\n",
    "library(devtools)\n",
    "library(tsne)\n",
    "library(usethis)\n",
    "library(SnowballC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devtools::install_github('bmschmidt/wordVectors', force=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(wordVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next portion, please be sure to provide the correct file name. This path2file currently directs the code to the documents concerning Latin America during the Cold War. If you would rather analyze the European files, input \"data/frusEU\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2file <- \"data/frusLA\"\n",
    "fileList <- list.files(path2file,full.names = TRUE) \n",
    "\n",
    "readTextFiles <- function(file) { \n",
    "  message(file)\n",
    "  rawText = paste(scan(file, sep=\"\\n\",what=\"raw\",strip.white = TRUE))\n",
    "  output = tibble(filename=gsub(path2file,\"\",file),text=rawText) %>% \n",
    "    group_by(filename) %>% \n",
    "    summarise(text = paste(rawText, collapse = \" \"))\n",
    "  return(output)\n",
    "}\n",
    "\n",
    "combinedTexts <- tibble(filename=fileList) %>% \n",
    "  group_by(filename) %>% \n",
    "  do(readTextFiles(.$filename)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next chunk of code, users should rename the \"file_name\" to describe what model they are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseFile <- \"file_name\"\n",
    "w2vInput <- paste(\"data/\",baseFile,\".txt\", sep = \"\")\n",
    "w2vCleaned <- paste(\"data/\",baseFile,\"_cleaned.txt\", sep=\"\")\n",
    "w2vBin <- paste(\"data/\",baseFile,\".bin\", sep=\"\")\n",
    "combinedTexts$text %>% write_lines(w2vInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model is based on the parameters that are selected prior to running the code. Word embedding models allow you to “choose how expansive you want the explored space to be” (Schmidt 2015). Tuning the parameters results in a greater accuracy depending on the analysis you are intending to complete. In order to test the usage of terms in the corpus, I tested a large variety of parameters in order to determine the accuracy of each model. For each set of parameters, both the Latin American and European corpora were tested and were run through six iterations on each corpus. Ultimately, there were six sets of parameters and therefore 62 models created in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THREADS <- 3\n",
    "\n",
    "prep_word2vec(origin=w2vInput,destination=w2vCleaned,lowercase=T,bundle_ngrams=1)\n",
    "\n",
    "if (!file.exists(w2vBin)) {\n",
    "  w2vModel <- train_word2vec(\n",
    "    w2vCleaned,\n",
    "    output_file=w2vBin,\n",
    "    vectors=100,\n",
    "    threads=THREADS,\n",
    "    window=10, iter=10, negative_samples=5\n",
    "  )\n",
    "} else {\n",
    "  w2vModel <- read.vectors(w2vBin)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you choose to read in an existing .bin file, please see the next chunk of code. For example, I have included the .bin files for both Latin America documents and European documents that I use in the later analysis. If you would like to explore those documents instead of waitinig for a model to run, either input \"LA6a.bin\" for the corpus on Latin America, or \"EU6a.bin\" for the European-related corpus, replacing \"file_name.bin\". To use this code chunk, please remove the \"#\" before the code. \"#\" acts as a way to comment in the code, thus rendering the code inactive, if the user chooses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2vModel <- read.vectors(\"data/LA6a.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an image of the model and some sample queries with terms of gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vModel %>% plot(perplexity=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vModel %>% closest_to(\"girl\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vModel %>% closest_to(~\"girl\"+\"woman\"+\"girls\"+\"women\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters also present interesting representations of the model. As you can see by running the next chunk of code, clusters produce results that may be more predictable if you know the corpus well. For example, the corpus on Latin America in the Cold War produces clusters regarding regional resources, coastal fishiing, and more categories regarding hemispheric relations.\n",
    "Please change \"name_of_your_query\" to a file name you would prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers <- 150\n",
    "clustering <- kmeans(w2vModel,centers=centers,iter.max = 40)\n",
    "\n",
    "sapply(sample(1:centers, 10), function(n) {\n",
    "  names(clustering$cluster[clustering$cluster==n][1:15])\n",
    "})\n",
    "\n",
    "w2vExport <-sapply(sample(1:centers,150),function(n) {\n",
    "  names(clustering$cluster[clustering$cluster==n][1:15])\n",
    "})\n",
    "\n",
    "write.csv(file=\"output/name_of_your_query.csv\", x=w2vExport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this project were extracted from the model iteration with the highest rate of similarity. The original code includes a short method for validating the model. I have included five more word pairs that are good representations of the corpus. These were instrumental in determining which iterations of the model were the strongest representations of the corpus. In the <a href=\"https://github.com/ccloutier312/CloutierDHProject/blob/main/Analysis.ipynb\">analysis</a>, I use a single set of parameters on both corpora discussed. The files using these parameters are titled “LA6a” and \"EU6a\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list  = list.files(pattern=\"*.bin$\", recursive=TRUE)\n",
    "\n",
    "rownames <- c()\n",
    "\n",
    "data_frame <- data.frame()\n",
    "data = list(c(\"away\", \"off\"),\n",
    "            c(\"before\", \"after\"),\n",
    "            c(\"cause\", \"effects\"),\n",
    "            c(\"children\", \"parents\"),\n",
    "            c(\"come\", \"go\"),\n",
    "            c(\"day\", \"night\"),\n",
    "            c(\"first\", \"second\"),\n",
    "            c(\"good\", \"bad\"),\n",
    "            c(\"last\", \"first\"),\n",
    "            c(\"kind\", \"sort\"),\n",
    "            c(\"leave\", \"quit\"),\n",
    "            c(\"life\", \"death\"),\n",
    "            c(\"girl\", \"boy\"),\n",
    "            c(\"little\", \"small\"),\n",
    "            c(\"oil\", \"petroleum\"),\n",
    "            c(\"state\", \"department\"),\n",
    "            c(\"confidential\", \"secret\"),\n",
    "            c(\"east\", \"west\"),\n",
    "            c(\"aid\", \"assistance\"))\n",
    "\n",
    "\n",
    "data_list = list()\n",
    "\n",
    "for(fn in files_list) {\n",
    "  \n",
    "  wwp_model = read.vectors(fn)\n",
    "  sims <- c()\n",
    "  for(pairs in data)\n",
    "  {\n",
    "    vector1 <- c()\n",
    "    for(x in wwp_model[[pairs[1]]]) {\n",
    "      vector1 <- c(vector1, x)\n",
    "    }\n",
    "    \n",
    "    vector2 <- c()\n",
    "    for(x in wwp_model[[pairs[2]]]) {\n",
    "      vector2 <- c(vector2, x)\n",
    "    }\n",
    "    \n",
    "    sims <- c(sims, cosine(vector1, vector2))\n",
    "    f_name <- strsplit(fn, \"/\")[[1]][[2]]\n",
    "    data_list[[f_name]] <- sims\n",
    "  }\n",
    "  \n",
    "}\n",
    "\n",
    "for(pairs in data) {\n",
    "  rownames <- c(rownames, paste(pairs[1], pairs[2], sep=\"-\"))\n",
    "}\n",
    "\n",
    "results <- structure(data_list,\n",
    "                     class     = \"data.frame\",\n",
    "                     row.names = rownames\n",
    ")\n",
    "\n",
    "write.csv(file=\"output/model-test-results.csv\", x=results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
